{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Treino e teste de modelos treinados com o algortitmo AlphaZero - <b>Go</b>\n",
    "\n",
    "### - Inspirado no vídeo do freeCodeCamp: https://www.youtube.com/watch?v=wuSQpLinRB4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports necessários para o funcionamento do código.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import go\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import torch\n",
    "import math\n",
    "from torch.autograd import Variable\n",
    "from tqdm.notebook import trange"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementação da Rede Neural Convolucional (CNN) que será utilizada para a predição de probabilidades de jogadas e valores de estados "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, game, num_resBlocks, num_hidden, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Inicialização da rede\n",
    "        self.device = device\n",
    "        \n",
    "        # Camada inicial da rede (primeiro bloco)\n",
    "        self.startBlock = nn.Sequential(\n",
    "            nn.Conv2d(3, num_hidden, kernel_size=3, padding=1),  # Convolução 2D com ativação ReLU\n",
    "            nn.BatchNorm2d(num_hidden),  # Normalização por lotes\n",
    "            nn.ReLU()  # Ativação ReLU\n",
    "        )\n",
    "        \n",
    "        # Bloco principal contendo vários blocos residuais\n",
    "        self.backBone = nn.ModuleList(\n",
    "            [ResBlock(num_hidden) for i in range(num_resBlocks)]  # Lista de blocos residuais\n",
    "        )\n",
    "        \n",
    "        # Cabeça de política (saída para ações)\n",
    "        self.policyHead = nn.Sequential(\n",
    "            nn.Conv2d(num_hidden, 32, kernel_size=3, padding=1),  # Convolução 2D com ativação ReLU\n",
    "            nn.BatchNorm2d(32),  # Normalização por lotes\n",
    "            nn.ReLU(),  # Ativação ReLU\n",
    "            nn.Flatten(),  # Aplanamento dos dados\n",
    "            nn.Linear(32 * game.row_count * game.column_count, game.action_size)  # Camada totalmente conectada\n",
    "        )\n",
    "        \n",
    "        # Saída para avaliação de estado\n",
    "        self.valueHead = nn.Sequential(\n",
    "            nn.Conv2d(num_hidden, 3, kernel_size=3, padding=1),  # Convolução 2D com ativação ReLU\n",
    "            nn.BatchNorm2d(3),  # Normalização por lotes\n",
    "            nn.ReLU(),  # Ativação ReLU\n",
    "            nn.Flatten(),  # Aplanamento dos dados\n",
    "            nn.Linear(3 * game.row_count * game.column_count, 1),  # Camada fully connected\n",
    "            nn.Tanh()  # Função de ativação tangente hiperbólica\n",
    "        )\n",
    "        \n",
    "        # Configuração do dispositivo\n",
    "        self.to(device)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Propagação dos dados através da rede\n",
    "        x = self.startBlock(x)\n",
    "        for resBlock in self.backBone:\n",
    "            x = resBlock(x)\n",
    "        policy = self.policyHead(x)  # Saída da cabeça de política\n",
    "        value = self.valueHead(x)  # Saída da cabeça de valor\n",
    "        return policy, value\n",
    "\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, num_hidden):\n",
    "        super().__init__()\n",
    "        # Definição do bloco residual\n",
    "        self.conv1 = nn.Conv2d(num_hidden, num_hidden, kernel_size=3, padding=1)  # Primeira camada convolucional\n",
    "        self.bn1 = nn.BatchNorm2d(num_hidden)  # Normalização por lotes para a primeira camada convolucional\n",
    "        self.conv2 = nn.Conv2d(num_hidden, num_hidden, kernel_size=3, padding=1)  # Segunda camada convolucional\n",
    "        self.bn2 = nn.BatchNorm2d(num_hidden)  # Normalização por lotes para a segunda camada convolucional\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Propagação dos dados através do bloco residual\n",
    "        residual = x\n",
    "        x = F.relu(self.bn1(self.conv1(x)))  # Ativação ReLU após a primeira camada convolucional\n",
    "        x = self.bn2(self.conv2(x))  # Normalização por lotes após a segunda camada convolucional\n",
    "        x += residual  # Adição do atalho (conexão residual)\n",
    "        x = F.relu(x)  # Ativação ReLU final\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementação do algoritmo MCTS (Monte Carlo Tree Search) que será utilizado para a escolha de jogadas\n",
    "Neste caso é o MCTS Paralelo, que utiliza múltiplas threads para simular vários jogos simultaneamente durante o treino para escolher a melhor jogada possível"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Classe auxiliar para armazenar os dados de um jogo\n",
    "class Node:\n",
    "    def __init__(self, game, args, state, parent=None, action_taken=None, prior=0, visit_count=0):\n",
    "        # Inicialização de um nó no MCTS\n",
    "        self.game = game\n",
    "        self.args = args\n",
    "        self.state = state\n",
    "        self.parent = parent\n",
    "        self.action_taken = action_taken\n",
    "        self.prior = prior\n",
    "        \n",
    "        self.children = []\n",
    "        \n",
    "        self.visit_count = visit_count\n",
    "        self.value_sum = 0\n",
    "        \n",
    "    def is_fully_expanded(self):\n",
    "        return len(self.children) > 0\n",
    "    \n",
    "    def select(self):\n",
    "        # Seleção do melhor filho com base no UCB (Upper Confidence Bound)\n",
    "        best_child = None\n",
    "        best_ucb = -np.inf\n",
    "        \n",
    "        for child in self.children:\n",
    "            ucb = self.get_ucb(child)\n",
    "            if ucb > best_ucb:\n",
    "                best_child = child\n",
    "                best_ucb = ucb\n",
    "                \n",
    "        return best_child\n",
    "    \n",
    "    def get_ucb(self, child):\n",
    "        # Cálculo do UCB para um filho específico\n",
    "        if child.visit_count == 0:\n",
    "            q_value = 0\n",
    "        else:\n",
    "            q_value = 1 - ((child.value_sum / child.visit_count) + 1) / 2\n",
    "        return q_value + self.args['C'] * (math.sqrt(self.visit_count) / (child.visit_count + 1)) * child.prior\n",
    "    \n",
    "    def expand(self, policy):\n",
    "        # Expansão do nó com base na policy de probabilidade\n",
    "        for action, prob in enumerate(policy):\n",
    "            if prob > 0:\n",
    "                child_state = self.state.copy()\n",
    "                child_state = self.game.get_next_state(child_state, action, 1)\n",
    "                child_state = self.game.change_perspective(child_state, player=-1)\n",
    "\n",
    "                child = Node(self.game, self.args, child_state, self, action, prob)\n",
    "                self.children.append(child)\n",
    "                \n",
    "        return child\n",
    "            \n",
    "    def backpropagate(self, value):\n",
    "        # Backpropagation do valor do nó até a raiz\n",
    "        self.value_sum += value\n",
    "        self.visit_count += 1\n",
    "        \n",
    "        value = self.game.get_opponent_value(value)\n",
    "        if self.parent is not None:\n",
    "            self.parent.backpropagate(value)  \n",
    "\n",
    "class MCTSParallel:\n",
    "    def __init__(self, game, args, model):\n",
    "        # Inicialização do MCTS\n",
    "        self.game = game\n",
    "        self.args = args\n",
    "        self.model = model\n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def search(self, state, spGames, pre=None):\n",
    "        # Realiza uma busca MCTS para vários jogos simultaneamente\n",
    "        policy, _ = self.model(\n",
    "            torch.tensor(self.game.get_encoded_state(state), device=self.model.device)\n",
    "        )\n",
    "        policy = torch.softmax(policy, axis=1).cpu().numpy()\n",
    "        policy = (1 - self.args['dirichlet_epsilon']) * policy + self.args['dirichlet_epsilon'] \\\n",
    "            * np.random.dirichlet([self.args['dirichlet_alpha']] * self.game.action_size, size=policy.shape[0])\n",
    "        \n",
    "        # Inicializa os nós raiz para cada jogo\n",
    "        for i, spg in enumerate(spGames):\n",
    "            p=None\n",
    "            spg_policy = policy[i]\n",
    "            if pre[0] is not None:\n",
    "                p= pre[i]\n",
    "            valid_moves = self.game.get_valid_moves(state[i], p)\n",
    "            spg_policy *= valid_moves\n",
    "            spg_policy /= np.sum(spg_policy)\n",
    "\n",
    "            spg.root = Node(self.game, self.args, state[i], visit_count=1)\n",
    "            spg.root.expand(spg_policy)\n",
    "        \n",
    "        for search in range(self.args['num_searches']):\n",
    "            for spg in spGames:\n",
    "                spg.node = None\n",
    "                node = spg.root\n",
    "                pas=False  # Variável para indicar se ocorreu um passe\n",
    "                pre=None  # Estado anterior do jogo\n",
    "\n",
    "                while node.is_fully_expanded():\n",
    "                    node = node.select()\n",
    "\n",
    "                # Verifica se ocorreu um passe\n",
    "                if node.parent is not None:\n",
    "                    if (node.parent.action_taken == self.game.action_size - 1 and node.action_taken == self.game.action_size - 1):\n",
    "                        pas = True \n",
    "\n",
    "                # Verifica se o jogo termino\n",
    "                value, is_terminal = self.game.get_value_and_terminated(node.state, pas)\n",
    "                value = self.game.get_opponent_value(value)\n",
    "\n",
    "                if is_terminal:\n",
    "                    node.backpropagate(value)\n",
    "                else:\n",
    "                    spg.node = node\n",
    "            # Realiza a expansão dos nós\n",
    "            expandable_spGames = [mappingIdx for mappingIdx in range(len(spGames)) if spGames[mappingIdx].node is not None]\n",
    "\n",
    "            # Realiza a expansão dos nós\n",
    "            if len(expandable_spGames) > 0:\n",
    "                state = np.stack([spGames[mappingIdx].node.state for mappingIdx in expandable_spGames])\n",
    "                \n",
    "                policy, value = self.model(\n",
    "                    torch.tensor(self.game.get_encoded_state(state), device=self.model.device) # Codifica o estado do jogo\n",
    "                )\n",
    "                # Decodifica a policy e o valor\n",
    "                policy = torch.softmax(policy, axis=1).cpu().numpy()\n",
    "                value = value.cpu().numpy()\n",
    "\n",
    "            # Atualiza os nós expandidos\n",
    "            for i, mappingIdx in enumerate(expandable_spGames):\n",
    "                node = spGames[mappingIdx].node\n",
    "                spg_policy, spg_value = policy[i], value[i]\n",
    "\n",
    "                if node.parent is not  None:\n",
    "                    if node.parent.parent  is not None:\n",
    "                        pre= node.parent.parent.state\n",
    "\n",
    "                valid_moves = self.game.get_valid_moves(node.state, pre)\n",
    "                spg_policy *= valid_moves\n",
    "                spg_policy /= np.sum(spg_policy)\n",
    "                pre=None\n",
    "\n",
    "                node.expand(spg_policy)\n",
    "                node.backpropagate(spg_value)  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MCTS normal para testar os modelos treinados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCTS:\n",
    "    def __init__(self, game, args, model):\n",
    "        # Inicialização do MCTS\n",
    "        self.game = game\n",
    "        self.args = args\n",
    "        self.model = model\n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def search(self, state, pre=None):\n",
    "        # Busca MCTS para selecionar a melhor ação dado um estado do jogo\n",
    "        root = Node(self.game, self.args, state, visit_count=1)\n",
    "        \n",
    "        # Obtenção da política de probabilidade a partir da rede\n",
    "        policy, _ = self.model(\n",
    "            torch.tensor(self.game.get_encoded_state(state), device=self.model.device).unsqueeze(0)\n",
    "        )\n",
    "        policy = torch.softmax(policy, axis=1).squeeze(0).cpu().numpy()\n",
    "        \n",
    "        # Adição de ruído de Dirichlet para exploração estocástica\n",
    "        policy = (1 - self.args['dirichlet_epsilon']) * policy + self.args['dirichlet_epsilon'] \\\n",
    "            * np.random.dirichlet([self.args['dirichlet_alpha']] * self.game.action_size)\n",
    "        \n",
    "        # Aplicação das jogadas válidas ao nó raiz\n",
    "        valid_moves = self.game.get_valid_moves(root.state, pre)\n",
    "        policy *= valid_moves\n",
    "        policy /= np.sum(policy)\n",
    "        root.expand(policy)\n",
    "\n",
    "        # Realização de iterações de busca MCTS\n",
    "        for search in range(self.args['num_searches']):\n",
    "            node = root\n",
    "            pas = False  # Variável para indicar se ocorreu um pass\n",
    "            pre = None  # Estado anterior do jogo\n",
    "            \n",
    "            while node.is_fully_expanded():\n",
    "                node = node.select()\n",
    "\n",
    "            if node.parent is not None:\n",
    "                if (node.parent.action_taken == self.game.action_size-1 and node.action_taken == self.game.action_size-1):\n",
    "                    pas = True \n",
    "\n",
    "            # Avaliação do valor e se é um state terminal\n",
    "            value, is_terminal = self.game.get_value_and_terminated(node.state, pas)\n",
    "            value = self.game.get_opponent_value(value)\n",
    "\n",
    "            if not is_terminal:\n",
    "                # Obtenção da policy de probabilidade e valor da rede para o nó atual\n",
    "                policy, value = self.model(\n",
    "                    torch.tensor(self.game.get_encoded_state(node.state), device=self.model.device).unsqueeze(0)\n",
    "                )\n",
    "                policy = torch.softmax(policy, axis=1).squeeze(0).cpu().numpy()\n",
    "                \n",
    "                # Atualização do estado anterior\n",
    "                if node.parent is not None:\n",
    "                    if node.parent.parent is not None:\n",
    "                        pre = node.parent.parent.state\n",
    "\n",
    "                # Aplicação das jogadas válidas ao nó atual\n",
    "                valid_moves = self.game.get_valid_moves(node.state, pre)\n",
    "                policy *= valid_moves\n",
    "                policy /= np.sum(policy)\n",
    "\n",
    "                pre = None\n",
    "                value = value.item()\n",
    "\n",
    "                # Expansão do nó e backpropagation do valor\n",
    "                node.expand(policy)\n",
    "                \n",
    "            node.backpropagate(value)    \n",
    "            \n",
    "        # Cálculo das probabilidades de ação normalizadas a partir dos visit_count dos filhos do nó raiz\n",
    "        action_probs = np.zeros(self.game.action_size)\n",
    "        for child in root.children:\n",
    "            action_probs[child.action_taken] = child.visit_count\n",
    "        action_probs /= np.sum(action_probs)\n",
    "        return action_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementação do algoritmo AlphaZero que utiliza a CNN e o MCTS para treinar um modelo de IA para jogar Go\n",
    "\n",
    "Neste caso é o AlphaZero Paralelo, que utiliza múltiplas threads para simular diversos jogos ao mesmo tempo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlphaZeroParallel:\n",
    "    def __init__(self, model, optimizer, game, args):\n",
    "        # Inicialização do AlphaZero para treino paralelo\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.game = game\n",
    "        self.args = args\n",
    "        self.mcts = MCTSParallel(game, args, model)\n",
    "        \n",
    "    def selfPlay(self):\n",
    "        # Simulação de partidas para recolha de dados de treino\n",
    "        return_memory = []\n",
    "        player = 1\n",
    "        spGames = [SPG(self.game) for spg in range(self.args['num_parallel_games'])]\n",
    "        pre_states= [None]\n",
    "\n",
    "        \n",
    "        # Realização de jogadas até que todos os jogos terminem\n",
    "        while len(spGames) > 0:\n",
    "            print(len(spGames))\n",
    "            states = np.stack([spg.state for spg in spGames])\n",
    "            neutral_states = self.game.change_perspective(states, player)\n",
    "            if(spGames[0].prev[player] is not None):\n",
    "                pre_states= np.stack([spg.prev[player]for spg in spGames])\n",
    "            self.mcts.search(neutral_states, spGames,pre_states)\n",
    "            # Realização de jogadas para cada jogo\n",
    "            for i in range(len(spGames))[::-1]:\n",
    "                spg = spGames[i]\n",
    "                \n",
    "                action_probs = np.zeros(self.game.action_size)\n",
    "                for child in spg.root.children:\n",
    "                    action_probs[child.action_taken] = child.visit_count\n",
    "                action_probs /= np.sum(action_probs)\n",
    "\n",
    "                spg.memory.append((spg.root.state, action_probs, player))\n",
    "                # Realização de jogadas aleatórias com base na temperatura\n",
    "                temperature_action_probs = action_probs ** (1 / self.args['temperature'])\n",
    "                temperature_action_probs /= np.sum(temperature_action_probs)\n",
    "                action = np.random.choice(self.game.action_size, p=temperature_action_probs)\n",
    "\n",
    "                spg.prev[player]= spg.state\n",
    "                spg.state = self.game.get_next_state(spg.state, action, player)\n",
    "                if action == self.game.action_size - 1:\n",
    "                    spg.pas += 1\n",
    "                    if spg.pas == 2:\n",
    "                        spg.passe = True\n",
    "                else:\n",
    "                    spg.pas = 0\n",
    "\n",
    "                value, is_terminal = self.game.get_value_and_terminated(spg.state, spg.passe)\n",
    "                spg.passe = False\n",
    "                # Adição dos dados de treino\n",
    "                if is_terminal:\n",
    "                    for hist_neutral_state, hist_action_probs, hist_player in spg.memory:\n",
    "                        hist_outcome = value if hist_player == player else self.game.get_opponent_value(value)\n",
    "                        return_memory.append((\n",
    "                            self.game.get_encoded_state(hist_neutral_state),\n",
    "                            hist_action_probs,\n",
    "                            hist_outcome\n",
    "                        ))\n",
    "                    del spGames[i]\n",
    "            \n",
    "            player = self.game.get_opponent(player)\n",
    "        \n",
    "        return return_memory\n",
    "                \n",
    "    def train(self, memory):\n",
    "        # Treino da rede neuronal com os dados recolhidos\n",
    "        random.shuffle(memory)\n",
    "        for batchIdx in range(0, len(memory), self.args['batch_size']):\n",
    "            sample = memory[batchIdx:min(len(memory) - 1, batchIdx + self.args['batch_size'])]\n",
    "            state, policy_targets, value_targets = zip(*sample)\n",
    "            \n",
    "            state, policy_targets, value_targets = np.array(state), np.array(policy_targets), np.array(value_targets).reshape(-1, 1)\n",
    "            \n",
    "            state = torch.tensor(state, dtype=torch.float32, device=self.model.device)\n",
    "            policy_targets = torch.tensor(policy_targets, dtype=torch.float32, device=self.model.device)\n",
    "            value_targets = torch.tensor(value_targets, dtype=torch.float32, device=self.model.device)\n",
    "            \n",
    "            out_policy, out_value = self.model(state)\n",
    "            \n",
    "            policy_loss = F.cross_entropy(out_policy, policy_targets)\n",
    "            value_loss = F.mse_loss(out_value, value_targets)\n",
    "            loss = policy_loss + value_loss\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "    \n",
    "    def learn(self):\n",
    "        # Treino iterativo do modelo de acordo com os parâmetros definidos\n",
    "        for iteration in range(self.args['num_iterations']):\n",
    "            memory = []\n",
    "            \n",
    "            self.model.eval()\n",
    "            for selfPlay_iteration in trange(self.args['num_selfPlay_iterations'] // self.args['num_parallel_games']):\n",
    "                memory += self.selfPlay()\n",
    "                \n",
    "            self.model.train()\n",
    "            for epoch in trange(self.args['num_epochs']):\n",
    "                self.train(memory)\n",
    "            \n",
    "            # Guarda o modelo e o optimizer\n",
    "            torch.save(self.model.state_dict(), f\"model_7_{iteration}_{self.game}.pt\")\n",
    "            torch.save(self.optimizer.state_dict(), f\"optimizer_7_{iteration}_{self.game}.pt\")\n",
    "\n",
    "class SPG:\n",
    "    def __init__(self, game):\n",
    "        # Inicialização de uma instância de jogo para autojogo\n",
    "        self.state = game.get_initial_state()\n",
    "        self.memory = []\n",
    "        self.root = None\n",
    "        self.node = None\n",
    "        self.passe = False\n",
    "        self.pas = 0\n",
    "        self.prev=[None,None]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AlphaZero normal (não usamos na nossa implementação)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlphaZero:\n",
    "    def __init__(self, model, optimizer, game, args):\n",
    "        # Inicialização do AlphaZero para treino\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.game = game\n",
    "        self.args = args\n",
    "        self.mcts = MCTS(game, args, model)\n",
    "        \n",
    "    def selfPlay(self):\n",
    "        # Simulação de partidas para recolha de dados de treino\n",
    "        memory = []\n",
    "        player = 1\n",
    "        state = self.game.get_initial_state()\n",
    "        pas = 0\n",
    "        \n",
    "        while True:\n",
    "            passe = False\n",
    "            neutral_state = self.game.change_perspective(state, player)\n",
    "            action_probs = self.mcts.search(neutral_state)\n",
    "            \n",
    "            memory.append((neutral_state, action_probs, player))\n",
    "            \n",
    "            temperature_action_probs = action_probs ** (1 / self.args['temperature'])\n",
    "            temperature_action_probs /= np.sum(temperature_action_probs)\n",
    "            action = np.random.choice(self.game.action_size, p=temperature_action_probs)\n",
    "            \n",
    "            state = self.game.get_next_state(state, action, player)\n",
    "            \n",
    "            if action == self.game.action_size - 1:\n",
    "                pas += 1\n",
    "                if pas == 2:\n",
    "                    passe = True\n",
    "            else:\n",
    "                pas = 0\n",
    "            \n",
    "            value, is_terminal = self.game.get_value_and_terminated(state, passe)\n",
    "            \n",
    "            if is_terminal:\n",
    "                return_memory = []\n",
    "                for hist_neutral_state, hist_action_probs, hist_player in memory:\n",
    "                    hist_outcome = value if hist_player == player else self.game.get_opponent_value(value)\n",
    "                    return_memory.append((\n",
    "                        self.game.get_encoded_state(hist_neutral_state),\n",
    "                        hist_action_probs,\n",
    "                        hist_outcome\n",
    "                    ))\n",
    "                return return_memory\n",
    "            \n",
    "            player = self.game.get_opponent(player)\n",
    "                \n",
    "    def train(self, memory):\n",
    "        # Treino da rede neuronal com os dados recolhidos\n",
    "        random.shuffle(memory)\n",
    "        for batchIdx in range(0, len(memory), self.args['batch_size']):\n",
    "            sample = memory[batchIdx:min(len(memory) - 1, batchIdx + self.args['batch_size'])]\n",
    "            state, policy_targets, value_targets = zip(*sample)\n",
    "            \n",
    "            state, policy_targets, value_targets = np.array(state), np.array(policy_targets), np.array(value_targets).reshape(-1, 1)\n",
    "            \n",
    "            state = torch.tensor(state, dtype=torch.float32, device=self.model.device)\n",
    "            policy_targets = torch.tensor(policy_targets, dtype=torch.float32, device=self.model.device)\n",
    "            value_targets = torch.tensor(value_targets, dtype=torch.float32, device=self.model.device)\n",
    "            \n",
    "            out_policy, out_value = self.model(state)\n",
    "            \n",
    "            policy_loss = F.cross_entropy(out_policy, policy_targets)\n",
    "            value_loss = F.mse_loss(out_value, value_targets)\n",
    "            loss = policy_loss + value_loss\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "    \n",
    "    def learn(self):\n",
    "        # Treino iterativo do modelo\n",
    "        for iteration in range(self.args['num_iterations']):\n",
    "            memory = []\n",
    "            \n",
    "            self.model.eval()\n",
    "            for selfPlay_iteration in trange(self.args['num_selfPlay_iterations']):\n",
    "                memory += self.selfPlay()\n",
    "                \n",
    "            self.model.train()\n",
    "            for epoch in trange(self.args['num_epochs']):\n",
    "                self.train(memory)\n",
    "            \n",
    "            torch.save(self.model.state_dict(), f\"new_model_7x7_{iteration}_{self.game}.pt\")\n",
    "            torch.save(self.optimizer.state_dict(), f\"new_optimizer_7x7_{iteration}_{self.game}.pt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classe auxiliar para a criação de um tabuleiro de Go e para a realização de jogadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Connect2Game:\n",
    "    def __init__(self,n):\n",
    "        # Inicialização do jogo\n",
    "        self.row_count = n\n",
    "        self.column_count = n\n",
    "        self.action_size = n*n+1\n",
    "        \n",
    "    def __repr__(self):\n",
    "        # Representação do jogo\n",
    "        return \"Go\"\n",
    "        \n",
    "    def get_initial_state(self):\n",
    "        # Retorna o tabuleiro inicial do jogo (vazio)\n",
    "        return np.zeros((self.row_count, self.column_count))\n",
    "    \n",
    "    def get_next_state(self, state, action, player):\n",
    "        # Retorna o tabuleiro após a jogada\n",
    "        b = go.GameState(state, play_idx=1)\n",
    "        b.turn=player\n",
    "        # Transforma a ação em coordenadas\n",
    "        row = action// self.column_count\n",
    "        col = action % self.column_count\n",
    "\n",
    "        if action == self.column_count**2:\n",
    "            boa= b.pass_turn()\n",
    "        else:\n",
    "            boa = b.move(row,col)\n",
    "        return boa.board\n",
    "    \n",
    "    def get_valid_moves(self, state,previous):\n",
    "        # Retorna as jogadas válidas\n",
    "        valid_moves = [0] * self.action_size\n",
    "        valid_moves[-1]=1\n",
    "        b = go.GameState(state, play_idx=1)\n",
    "        # Vai buscar o estado anterior\n",
    "        if previous is not None:\n",
    "            b.previous_boards[1]=previous\n",
    "        # Verifica as jogadas possíveis em coordenadas\n",
    "        possi=go.check_possible_moves(b)\n",
    "        # Transforma as coordenadas em ações\n",
    "        for i in possi:\n",
    "            action = i[0] * self.column_count + i[1]\n",
    "            valid_moves[action]=1\n",
    "        return valid_moves\n",
    "    \n",
    "    def get_value_and_terminated(self, state, pas):\n",
    "        # Retorna o winner e se é um estado terminal\n",
    "        b = go.GameState(state, play_idx=1)\n",
    "        if pas:\n",
    "            b.pass_count = 2\n",
    "            self.game_over = True\n",
    "        _ , terminated =b.get_value_and_terminated(b)\n",
    "        value = self.winner(state)\n",
    "        return value, terminated\n",
    "    \n",
    "    def winner(self, state):\n",
    "        # Retorna o vencedor do jogo\n",
    "        scores = self.scores(state)\n",
    "        value = value_scores(scores)\n",
    "        return value\n",
    "    \n",
    "    def get_opponent(self, player):\n",
    "        # Retorna o oponente\n",
    "        return -player\n",
    "    \n",
    "    def get_opponent_value(self, value):\n",
    "        # Retorna o valor do oponente\n",
    "        return -value\n",
    "    \n",
    "    def change_perspective(self, state, player):\n",
    "        # Retorna o tabuleiro com a perspetiva do jogador\n",
    "        return state * player\n",
    "    \n",
    "    def get_encoded_state(self, state):\n",
    "        # Retorna o estado codificado\n",
    "        encoded_state = np.stack(\n",
    "            (state == -1, state == 0, state == 1)\n",
    "        ).astype(np.float32)\n",
    "        \n",
    "        if len(state.shape) == 3:\n",
    "            encoded_state = np.swapaxes(encoded_state, 0, 1)\n",
    "        \n",
    "        return encoded_state\n",
    "    \n",
    "    def scores(self, state):\n",
    "        # retorna os scores do jogo\n",
    "        b = go.GameState(state, play_idx=1)\n",
    "        return b.get_scores()\n",
    "    \n",
    "# Função auxiliar para calcular o valor do jogo\n",
    "def value_scores(scores):\n",
    "    if scores[1] > scores[-1]:\n",
    "        return 1\n",
    "    elif scores[1] < scores[-1]:\n",
    "        return -1\n",
    "    else:\n",
    "        return 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treino de um modelo de IA para jogar Go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicialização do jogo\n",
    "game = Connect2Game(7)\n",
    "\n",
    "# Definição do dispositivo a utilizar\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialização do modelo\n",
    "model = ResNet(game, 9, 128, device)\n",
    "\n",
    "# Definição do optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "\n",
    "# Definição dos parâmetros de treino\n",
    "args = {\n",
    "    'C': 2,\n",
    "    'num_searches': 300,\n",
    "    'num_iterations': 10,\n",
    "    'num_selfPlay_iterations': 50,\n",
    "    'num_parallel_games': 25,\n",
    "    'num_epochs': 10,\n",
    "    'batch_size': 128,\n",
    "    'temperature': 1.25,\n",
    "    'dirichlet_epsilon': 0.25,\n",
    "    'dirichlet_alpha': 0.3\n",
    "}\n",
    "\n",
    "# Inicialização do AlphaZero e treino\n",
    "alphaZero = AlphaZeroParallel(model, optimizer, game, args)\n",
    "alphaZero.learn()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testes com os modelos treinados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teste 1: Modelo vs Player Mau (só passa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_board = 7\n",
    "game = Connect2Game(n_board)\n",
    "player = 1\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "args = {\n",
    "    'C': 2,\n",
    "    'num_searches': 200,\n",
    "    'num_iterations': 5,\n",
    "    'num_selfPlay_iterations': 50,\n",
    "    'num_parallel_games': 25,\n",
    "    'num_epochs': 10,\n",
    "    'batch_size': 128,\n",
    "    'temperature': 1.25,\n",
    "    'dirichlet_epsilon': 0.25,\n",
    "    'dirichlet_alpha': 0.3\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "model = ResNet(game, 9,128,device)\n",
    "model.load_state_dict(torch.load(\"./Modelos/model_7_8_Go.pt\"))\n",
    "model.eval()\n",
    "\n",
    "mcts = MCTS(game, args, model)\n",
    "\n",
    "state = game.get_initial_state()\n",
    "previous_state = game.get_initial_state()\n",
    "\n",
    "\n",
    "pass_count = 0\n",
    "\n",
    "while True:\n",
    "    print(state)\n",
    "\n",
    "    if player != 1:\n",
    "        valid_moves = game.get_valid_moves(state, previous_state)\n",
    "        print(\"valid moves\", [i for i in range(game.action_size) if valid_moves[i] == 1])\n",
    "        action = 49 #int(input(\"Enter action: \"))\n",
    "        print(\"minha \" +str(action))\n",
    "        if action == game.action_size -1:\n",
    "            pass_count +=1\n",
    "        else:\n",
    "            pass_count = 0\n",
    "        print(\"pass count\", str( pass_count))\n",
    "        if valid_moves[action] == 0:\n",
    "            print(\"Invalid move\")\n",
    "            continue\n",
    "        \n",
    "        previous_state = state\n",
    "    \n",
    "    else:\n",
    "        neutral_state = game.change_perspective(state, player)\n",
    "        mtcs_probs = mcts.search(neutral_state)\n",
    "        action = np.argmax(mtcs_probs)\n",
    "        if action == game.action_size -1:\n",
    "            pass_count +=1\n",
    "        else:\n",
    "            pass_count = 0\n",
    "        print(\"bot\", str(action))\n",
    "        \n",
    "    state = game.get_next_state(state, action, player)\n",
    "    \n",
    "    value = game.winner(state)\n",
    "    scores = game.scores(state)\n",
    "    print(f\"scores:  {scores[1]}, {scores[-1]}\")\n",
    "    previous_state = state\n",
    "    \n",
    "    \n",
    "    if pass_count == 2:\n",
    "        print(state)\n",
    "        print(\"Game over\")\n",
    "        value = game.winner(state)\n",
    "        print(f\"scores: {scores[1]}, {scores[-1]}\")\n",
    "        if value != 0:\n",
    "            print(f\"{value} wins\")\n",
    "        else:\n",
    "            print(\"Draw\")\n",
    "        break\n",
    "    \n",
    "    player = game.get_opponent(player)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teste 2: Modelo com menos treino vs Modelo com mais treino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]]\n",
      "scores: 0, 5.5\n",
      "bot 1-  10\n",
      "[[0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]]\n",
      "scores: 49, 5.5\n",
      "bot 2-  22\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]]\n",
      "scores: 1, 6.5\n",
      "bot 1-  21\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1. -1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]]\n",
      "scores: 2, 6.5\n",
      "bot 2-  9\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0. -1.  1.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1. -1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]]\n",
      "scores: 2, 7.5\n",
      "bot 1-  8\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  1. -1.  1.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1. -1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]]\n",
      "scores: 3, 7.5\n",
      "bot 2-  20\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  1. -1.  1.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0. -1.]\n",
      " [ 1. -1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]]\n",
      "scores: 3, 8.5\n",
      "bot 1-  1\n",
      "[[ 0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  1. -1.  1.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0. -1.]\n",
      " [ 1. -1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]]\n",
      "scores: 4, 8.5\n",
      "bot 2-  28\n",
      "[[ 0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  1. -1.  1.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0. -1.]\n",
      " [ 1. -1.  0.  0.  0.  0.  0.]\n",
      " [-1.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]]\n",
      "scores: 4, 9.5\n",
      "bot 1-  30\n",
      "[[ 0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  1. -1.  1.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0. -1.]\n",
      " [ 1. -1.  0.  0.  0.  0.  0.]\n",
      " [-1.  0.  1.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]]\n",
      "scores: 5, 9.5\n",
      "bot 2-  14\n",
      "[[ 0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  1. -1.  1.  0.  0.  0.]\n",
      " [-1.  0.  0.  0.  0.  0. -1.]\n",
      " [ 0. -1.  0.  0.  0.  0.  0.]\n",
      " [-1.  0.  1.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]]\n",
      "scores: 4, 11.5\n",
      "bot 1-  46\n",
      "[[ 0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  1. -1.  1.  0.  0.  0.]\n",
      " [-1.  0.  0.  0.  0.  0. -1.]\n",
      " [ 0. -1.  0.  0.  0.  0.  0.]\n",
      " [-1.  0.  1.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  0.  0.]]\n",
      "scores: 5, 11.5\n",
      "bot 2-  7\n",
      "[[ 0.  1.  0.  0.  0.  0.  0.]\n",
      " [-1.  1. -1.  1.  0.  0.  0.]\n",
      " [-1.  0.  0.  0.  0.  0. -1.]\n",
      " [ 0. -1.  0.  0.  0.  0.  0.]\n",
      " [-1.  0.  1.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  0.  0.]]\n",
      "scores: 5, 12.5\n",
      "bot 1-  25\n",
      "[[ 0.  1.  0.  0.  0.  0.  0.]\n",
      " [-1.  1. -1.  1.  0.  0.  0.]\n",
      " [-1.  0.  0.  0.  0.  0. -1.]\n",
      " [ 0. -1.  0.  0.  1.  0.  0.]\n",
      " [-1.  0.  1.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  0.  0.]]\n",
      "scores: 6, 12.5\n",
      "bot 2-  23\n",
      "[[ 0.  1.  0.  0.  0.  0.  0.]\n",
      " [-1.  1. -1.  1.  0.  0.  0.]\n",
      " [-1.  0.  0.  0.  0.  0. -1.]\n",
      " [ 0. -1. -1.  0.  1.  0.  0.]\n",
      " [-1.  0.  1.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  0.  0.]]\n",
      "scores: 6, 13.5\n",
      "bot 1-  37\n",
      "[[ 0.  1.  0.  0.  0.  0.  0.]\n",
      " [-1.  1. -1.  1.  0.  0.  0.]\n",
      " [-1.  0.  0.  0.  0.  0. -1.]\n",
      " [ 0. -1. -1.  0.  1.  0.  0.]\n",
      " [-1.  0.  1.  0.  0.  0.  0.]\n",
      " [ 0.  0.  1.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  0.  0.]]\n",
      "scores: 7, 13.5\n",
      "bot 2-  17\n",
      "[[ 0.  1.  0.  0.  0.  0.  0.]\n",
      " [-1.  1. -1.  1.  0.  0.  0.]\n",
      " [-1.  0.  0. -1.  0.  0. -1.]\n",
      " [ 0. -1. -1.  0.  1.  0.  0.]\n",
      " [-1.  0.  1.  0.  0.  0.  0.]\n",
      " [ 0.  0.  1.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  0.  0.]]\n",
      "scores: 7, 14.5\n",
      "bot 1-  45\n",
      "[[ 0.  1.  0.  0.  0.  0.  0.]\n",
      " [-1.  1. -1.  1.  0.  0.  0.]\n",
      " [-1.  0.  0. -1.  0.  0. -1.]\n",
      " [ 0. -1. -1.  0.  1.  0.  0.]\n",
      " [-1.  0.  1.  0.  0.  0.  0.]\n",
      " [ 0.  0.  1.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  1.  0.  0.]]\n",
      "scores: 8, 14.5\n",
      "bot 2-  4\n",
      "[[ 0.  1.  0.  0. -1.  0.  0.]\n",
      " [-1.  1. -1.  1.  0.  0.  0.]\n",
      " [-1.  0.  0. -1.  0.  0. -1.]\n",
      " [ 0. -1. -1.  0.  1.  0.  0.]\n",
      " [-1.  0.  1.  0.  0.  0.  0.]\n",
      " [ 0.  0.  1.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  1.  0.  0.]]\n",
      "scores: 8, 15.5\n",
      "bot 1-  13\n",
      "[[ 0.  1.  0.  0. -1.  0.  0.]\n",
      " [-1.  1. -1.  1.  0.  0.  1.]\n",
      " [-1.  0.  0. -1.  0.  0. -1.]\n",
      " [ 0. -1. -1.  0.  1.  0.  0.]\n",
      " [-1.  0.  1.  0.  0.  0.  0.]\n",
      " [ 0.  0.  1.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  1.  0.  0.]]\n",
      "scores: 9, 15.5\n",
      "bot 2-  41\n",
      "[[ 0.  1.  0.  0. -1.  0.  0.]\n",
      " [-1.  1. -1.  1.  0.  0.  1.]\n",
      " [-1.  0.  0. -1.  0.  0. -1.]\n",
      " [ 0. -1. -1.  0.  1.  0.  0.]\n",
      " [-1.  0.  1.  0.  0.  0.  0.]\n",
      " [ 0.  0.  1.  0.  0.  0. -1.]\n",
      " [ 0.  0.  0.  1.  1.  0.  0.]]\n",
      "scores: 9, 16.5\n",
      "bot 1-  47\n",
      "[[ 0.  1.  0.  0. -1.  0.  0.]\n",
      " [-1.  1. -1.  1.  0.  0.  1.]\n",
      " [-1.  0.  0. -1.  0.  0. -1.]\n",
      " [ 0. -1. -1.  0.  1.  0.  0.]\n",
      " [-1.  0.  1.  0.  0.  0.  0.]\n",
      " [ 0.  0.  1.  0.  0.  0. -1.]\n",
      " [ 0.  0.  0.  1.  1.  1.  0.]]\n",
      "scores: 10, 16.5\n",
      "bot 2-  40\n",
      "[[ 0.  1.  0.  0. -1.  0.  0.]\n",
      " [-1.  1. -1.  1.  0.  0.  1.]\n",
      " [-1.  0.  0. -1.  0.  0. -1.]\n",
      " [ 0. -1. -1.  0.  1.  0.  0.]\n",
      " [-1.  0.  1.  0.  0.  0.  0.]\n",
      " [ 0.  0.  1.  0.  0. -1. -1.]\n",
      " [ 0.  0.  0.  1.  1.  1.  0.]]\n",
      "scores: 10, 17.5\n",
      "bot 1-  15\n",
      "[[ 0.  1.  0.  0. -1.  0.  0.]\n",
      " [-1.  1. -1.  1.  0.  0.  1.]\n",
      " [-1.  1.  0. -1.  0.  0. -1.]\n",
      " [ 0. -1. -1.  0.  1.  0.  0.]\n",
      " [-1.  0.  1.  0.  0.  0.  0.]\n",
      " [ 0.  0.  1.  0.  0. -1. -1.]\n",
      " [ 0.  0.  0.  1.  1.  1.  0.]]\n",
      "scores: 11, 17.5\n",
      "bot 2-  11\n",
      "[[ 0.  1.  0.  0. -1.  0.  0.]\n",
      " [-1.  1. -1.  1. -1.  0.  1.]\n",
      " [-1.  1.  0. -1.  0.  0. -1.]\n",
      " [ 0. -1. -1.  0.  1.  0.  0.]\n",
      " [-1.  0.  1.  0.  0.  0.  0.]\n",
      " [ 0.  0.  1.  0.  0. -1. -1.]\n",
      " [ 0.  0.  0.  1.  1.  1.  0.]]\n",
      "scores: 11, 18.5\n",
      "bot 1-  36\n",
      "[[ 0.  1.  0.  0. -1.  0.  0.]\n",
      " [-1.  1. -1.  1. -1.  0.  1.]\n",
      " [-1.  1.  0. -1.  0.  0. -1.]\n",
      " [ 0. -1. -1.  0.  1.  0.  0.]\n",
      " [-1.  0.  1.  0.  0.  0.  0.]\n",
      " [ 0.  1.  1.  0.  0. -1. -1.]\n",
      " [ 0.  0.  0.  1.  1.  1.  0.]]\n",
      "scores: 12, 18.5\n",
      "bot 2-  35\n",
      "[[ 0.  1.  0.  0. -1.  0.  0.]\n",
      " [-1.  1. -1.  1. -1.  0.  1.]\n",
      " [-1.  1.  0. -1.  0.  0. -1.]\n",
      " [ 0. -1. -1.  0.  1.  0.  0.]\n",
      " [-1.  0.  1.  0.  0.  0.  0.]\n",
      " [-1.  1.  1.  0.  0. -1. -1.]\n",
      " [ 0.  0.  0.  1.  1.  1.  0.]]\n",
      "scores: 12, 19.5\n",
      "bot 1-  32\n",
      "[[ 0.  1.  0.  0. -1.  0.  0.]\n",
      " [-1.  1. -1.  1. -1.  0.  1.]\n",
      " [-1.  1.  0. -1.  0.  0. -1.]\n",
      " [ 0. -1. -1.  0.  1.  0.  0.]\n",
      " [-1.  0.  1.  0.  1.  0.  0.]\n",
      " [-1.  1.  1.  0.  0. -1. -1.]\n",
      " [ 0.  0.  0.  1.  1.  1.  0.]]\n",
      "scores: 13, 19.5\n",
      "bot 2-  5\n",
      "[[ 0.  1.  0.  0. -1. -1.  0.]\n",
      " [-1.  1. -1.  1. -1.  0.  1.]\n",
      " [-1.  1.  0. -1.  0.  0. -1.]\n",
      " [ 0. -1. -1.  0.  1.  0.  0.]\n",
      " [-1.  0.  1.  0.  1.  0.  0.]\n",
      " [-1.  1.  1.  0.  0. -1. -1.]\n",
      " [ 0.  0.  0.  1.  1.  1.  0.]]\n",
      "scores: 13, 20.5\n",
      "bot 1-  38\n",
      "[[ 0.  1.  0.  0. -1. -1.  0.]\n",
      " [-1.  1. -1.  1. -1.  0.  1.]\n",
      " [-1.  1.  0. -1.  0.  0. -1.]\n",
      " [ 0. -1. -1.  0.  1.  0.  0.]\n",
      " [-1.  0.  1.  0.  1.  0.  0.]\n",
      " [-1.  1.  1.  1.  0. -1. -1.]\n",
      " [ 0.  0.  0.  1.  1.  1.  0.]]\n",
      "scores: 14, 20.5\n",
      "bot 2-  2\n",
      "[[ 0.  1. -1.  0. -1. -1.  0.]\n",
      " [-1.  1. -1.  1. -1.  0.  1.]\n",
      " [-1.  1.  0. -1.  0.  0. -1.]\n",
      " [ 0. -1. -1.  0.  1.  0.  0.]\n",
      " [-1.  0.  1.  0.  1.  0.  0.]\n",
      " [-1.  1.  1.  1.  0. -1. -1.]\n",
      " [ 0.  0.  0.  1.  1.  1.  0.]]\n",
      "scores: 14, 21.5\n",
      "bot 1-  26\n",
      "[[ 0.  1. -1.  0. -1. -1.  0.]\n",
      " [-1.  1. -1.  1. -1.  0.  1.]\n",
      " [-1.  1.  0. -1.  0.  0. -1.]\n",
      " [ 0. -1. -1.  0.  1.  1.  0.]\n",
      " [-1.  0.  1.  0.  1.  0.  0.]\n",
      " [-1.  1.  1.  1.  0. -1. -1.]\n",
      " [ 0.  0.  0.  1.  1.  1.  0.]]\n",
      "scores: 15, 21.5\n",
      "bot 2-  43\n",
      "[[ 0.  1. -1.  0. -1. -1.  0.]\n",
      " [-1.  1. -1.  1. -1.  0.  1.]\n",
      " [-1.  1.  0. -1.  0.  0. -1.]\n",
      " [ 0. -1. -1.  0.  1.  1.  0.]\n",
      " [-1.  0.  1.  0.  1.  0.  0.]\n",
      " [-1.  1.  1.  1.  0. -1. -1.]\n",
      " [ 0. -1.  0.  1.  1.  1.  0.]]\n",
      "scores: 15, 23.5\n",
      "bot 1-  34\n",
      "[[ 0.  1. -1.  0. -1. -1.  0.]\n",
      " [-1.  1. -1.  1. -1.  0.  1.]\n",
      " [-1.  1.  0. -1.  0.  0. -1.]\n",
      " [ 0. -1. -1.  0.  1.  1.  0.]\n",
      " [-1.  0.  1.  0.  1.  0.  1.]\n",
      " [-1.  1.  1.  1.  0. -1. -1.]\n",
      " [ 0. -1.  0.  1.  1.  1.  0.]]\n",
      "scores: 16, 23.5\n",
      "bot 2-  3\n",
      "[[ 0.  1. -1. -1. -1. -1.  0.]\n",
      " [-1.  1. -1.  0. -1.  0.  1.]\n",
      " [-1.  1.  0. -1.  0.  0. -1.]\n",
      " [ 0. -1. -1.  0.  1.  1.  0.]\n",
      " [-1.  0.  1.  0.  1.  0.  1.]\n",
      " [-1.  1.  1.  1.  0. -1. -1.]\n",
      " [ 0. -1.  0.  1.  1.  1.  0.]]\n",
      "scores: 15, 25.5\n",
      "bot 1-  44\n",
      "[[ 0.  1. -1. -1. -1. -1.  0.]\n",
      " [-1.  1. -1.  0. -1.  0.  1.]\n",
      " [-1.  1.  0. -1.  0.  0. -1.]\n",
      " [ 0. -1. -1.  0.  1.  1.  0.]\n",
      " [-1.  0.  1.  0.  1.  0.  1.]\n",
      " [-1.  1.  1.  1.  0. -1. -1.]\n",
      " [ 0. -1.  1.  1.  1.  1.  0.]]\n",
      "scores: 16, 25.5\n",
      "bot 2-  10\n",
      "[[ 0.  1. -1. -1. -1. -1.  0.]\n",
      " [-1.  1. -1. -1. -1.  0.  1.]\n",
      " [-1.  1.  0. -1.  0.  0. -1.]\n",
      " [ 0. -1. -1.  0.  1.  1.  0.]\n",
      " [-1.  0.  1.  0.  1.  0.  1.]\n",
      " [-1.  1.  1.  1.  0. -1. -1.]\n",
      " [ 0. -1.  1.  1.  1.  1.  0.]]\n",
      "scores: 16, 25.5\n",
      "bot 1-  49\n",
      "[[ 0.  1. -1. -1. -1. -1.  0.]\n",
      " [-1.  1. -1. -1. -1.  0.  1.]\n",
      " [-1.  1.  0. -1.  0.  0. -1.]\n",
      " [ 0. -1. -1.  0.  1.  1.  0.]\n",
      " [-1.  0.  1.  0.  1.  0.  1.]\n",
      " [-1.  1.  1.  1.  0. -1. -1.]\n",
      " [ 0. -1.  1.  1.  1.  1.  0.]]\n",
      "scores: 16, 25.5\n",
      "bot 2-  42\n",
      "[[ 0.  1. -1. -1. -1. -1.  0.]\n",
      " [-1.  1. -1. -1. -1.  0.  1.]\n",
      " [-1.  1.  0. -1.  0.  0. -1.]\n",
      " [ 0. -1. -1.  0.  1.  1.  0.]\n",
      " [-1.  0.  1.  0.  1.  0.  1.]\n",
      " [-1.  1.  1.  1.  0. -1. -1.]\n",
      " [-1. -1.  1.  1.  1.  1.  0.]]\n",
      "scores: 16, 25.5\n",
      "bot 1-  29\n",
      "[[ 0.  1. -1. -1. -1. -1.  0.]\n",
      " [-1.  1. -1. -1. -1.  0.  1.]\n",
      " [-1.  1.  0. -1.  0.  0. -1.]\n",
      " [ 0. -1. -1.  0.  1.  1.  0.]\n",
      " [-1.  1.  1.  0.  1.  0.  1.]\n",
      " [-1.  1.  1.  1.  0. -1. -1.]\n",
      " [-1. -1.  1.  1.  1.  1.  0.]]\n",
      "scores: 17, 25.5\n",
      "bot 2-  31\n",
      "[[ 0.  1. -1. -1. -1. -1.  0.]\n",
      " [-1.  1. -1. -1. -1.  0.  1.]\n",
      " [-1.  1.  0. -1.  0.  0. -1.]\n",
      " [ 0. -1. -1.  0.  1.  1.  0.]\n",
      " [-1.  1.  1. -1.  1.  0.  1.]\n",
      " [-1.  1.  1.  1.  0. -1. -1.]\n",
      " [-1. -1.  1.  1.  1.  1.  0.]]\n",
      "scores: 17, 26.5\n",
      "bot 1-  21\n",
      "[[ 0.  1. -1. -1. -1. -1.  0.]\n",
      " [-1.  1. -1. -1. -1.  0.  1.]\n",
      " [-1.  1.  0. -1.  0.  0. -1.]\n",
      " [ 1. -1. -1.  0.  1.  1.  0.]\n",
      " [ 0.  1.  1. -1.  1.  0.  1.]\n",
      " [ 0.  1.  1.  1.  0. -1. -1.]\n",
      " [ 0.  0.  1.  1.  1.  1.  0.]]\n",
      "scores: 22, 21.5\n",
      "bot 2-  28\n",
      "[[ 0.  1. -1. -1. -1. -1.  0.]\n",
      " [-1.  1. -1. -1. -1.  0.  1.]\n",
      " [-1.  1.  0. -1.  0.  0. -1.]\n",
      " [ 0. -1. -1.  0.  1.  1.  0.]\n",
      " [-1.  1.  1. -1.  1.  0.  1.]\n",
      " [ 0.  1.  1.  1.  0. -1. -1.]\n",
      " [ 0.  0.  1.  1.  1.  1.  0.]]\n",
      "scores: 17, 23.5\n",
      "bot 1-  18\n",
      "[[ 0.  1. -1. -1. -1. -1.  0.]\n",
      " [-1.  1. -1. -1. -1.  0.  1.]\n",
      " [-1.  1.  0. -1.  1.  0. -1.]\n",
      " [ 0. -1. -1.  0.  1.  1.  0.]\n",
      " [-1.  1.  1. -1.  1.  0.  1.]\n",
      " [ 0.  1.  1.  1.  0. -1. -1.]\n",
      " [ 0.  0.  1.  1.  1.  1.  0.]]\n",
      "scores: 18, 23.5\n",
      "bot 2-  16\n",
      "[[ 0.  1. -1. -1. -1. -1.  0.]\n",
      " [-1.  1. -1. -1. -1.  0.  1.]\n",
      " [-1.  1. -1. -1.  1.  0. -1.]\n",
      " [ 0. -1. -1.  0.  1.  1.  0.]\n",
      " [-1.  1.  1. -1.  1.  0.  1.]\n",
      " [ 0.  1.  1.  1.  0. -1. -1.]\n",
      " [ 0.  0.  1.  1.  1.  1.  0.]]\n",
      "scores: 18, 24.5\n",
      "bot 1-  39\n",
      "[[ 0.  1. -1. -1. -1. -1.  0.]\n",
      " [-1.  1. -1. -1. -1.  0.  1.]\n",
      " [-1.  1. -1. -1.  1.  0. -1.]\n",
      " [ 0. -1. -1.  0.  1.  1.  0.]\n",
      " [-1.  1.  1. -1.  1.  0.  1.]\n",
      " [ 0.  1.  1.  1.  1. -1. -1.]\n",
      " [ 0.  0.  1.  1.  1.  1.  0.]]\n",
      "scores: 19, 24.5\n",
      "bot 2-  49\n",
      "[[ 0.  1. -1. -1. -1. -1.  0.]\n",
      " [-1.  1. -1. -1. -1.  0.  1.]\n",
      " [-1.  1. -1. -1.  1.  0. -1.]\n",
      " [ 0. -1. -1.  0.  1.  1.  0.]\n",
      " [-1.  1.  1. -1.  1.  0.  1.]\n",
      " [ 0.  1.  1.  1.  1. -1. -1.]\n",
      " [ 0.  0.  1.  1.  1.  1.  0.]]\n",
      "scores: 19, 24.5\n",
      "bot 1-  27\n",
      "[[ 0.  1. -1. -1. -1. -1.  0.]\n",
      " [-1.  1. -1. -1. -1.  0.  1.]\n",
      " [-1.  1. -1. -1.  1.  0. -1.]\n",
      " [ 0. -1. -1.  0.  1.  1.  1.]\n",
      " [-1.  1.  1. -1.  1.  0.  1.]\n",
      " [ 0.  1.  1.  1.  1. -1. -1.]\n",
      " [ 0.  0.  1.  1.  1.  1.  0.]]\n",
      "scores: 20, 24.5\n",
      "bot 2-  49\n",
      "[[ 0.  1. -1. -1. -1. -1.  0.]\n",
      " [-1.  1. -1. -1. -1.  0.  1.]\n",
      " [-1.  1. -1. -1.  1.  0. -1.]\n",
      " [ 0. -1. -1.  0.  1.  1.  1.]\n",
      " [-1.  1.  1. -1.  1.  0.  1.]\n",
      " [ 0.  1.  1.  1.  1. -1. -1.]\n",
      " [ 0.  0.  1.  1.  1.  1.  0.]]\n",
      "scores: 20, 24.5\n",
      "bot 1-  35\n",
      "[[ 0.  1. -1. -1. -1. -1.  0.]\n",
      " [-1.  1. -1. -1. -1.  0.  1.]\n",
      " [-1.  1. -1. -1.  1.  0. -1.]\n",
      " [ 0. -1. -1.  0.  1.  1.  1.]\n",
      " [-1.  1.  1. -1.  1.  0.  1.]\n",
      " [ 1.  1.  1.  1.  1. -1. -1.]\n",
      " [ 0.  0.  1.  1.  1.  1.  0.]]\n",
      "scores: 23, 24.5\n",
      "bot 2-  49\n",
      "[[ 0.  1. -1. -1. -1. -1.  0.]\n",
      " [-1.  1. -1. -1. -1.  0.  1.]\n",
      " [-1.  1. -1. -1.  1.  0. -1.]\n",
      " [ 0. -1. -1.  0.  1.  1.  1.]\n",
      " [-1.  1.  1. -1.  1.  0.  1.]\n",
      " [ 1.  1.  1.  1.  1. -1. -1.]\n",
      " [ 0.  0.  1.  1.  1.  1.  0.]]\n",
      "scores: 23, 24.5\n",
      "bot 1-  49\n",
      "[[ 0.  1. -1. -1. -1. -1.  0.]\n",
      " [-1.  1. -1. -1. -1.  0.  1.]\n",
      " [-1.  1. -1. -1.  1.  0. -1.]\n",
      " [ 0. -1. -1.  0.  1.  1.  1.]\n",
      " [-1.  1.  1. -1.  1.  0.  1.]\n",
      " [ 1.  1.  1.  1.  1. -1. -1.]\n",
      " [ 0.  0.  1.  1.  1.  1.  0.]]\n",
      "Game over\n",
      "scores: 23, 24.5\n",
      "-1 wins\n"
     ]
    }
   ],
   "source": [
    "n_board = 7\n",
    "game = Connect2Game(n_board)\n",
    "player = 1\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = ResNet(game, 9,128,device)\n",
    "model.load_state_dict(torch.load(\"./Modelos/model_7_8_Go.pt\"))\n",
    "model.eval()\n",
    "\n",
    "mcts = MCTS(game, args, model)\n",
    "\n",
    "model2 = ResNet(game, 9,128,device)\n",
    "model2.load_state_dict(torch.load(\"./Modelos/model_7_9_Go.pt\"))\n",
    "model2.eval()\n",
    "\n",
    "mcts2 = MCTS(game, args, model2)\n",
    "\n",
    "state = game.get_initial_state()\n",
    "previous_state = game.get_initial_state()\n",
    "\n",
    "\n",
    "pass_count = 0\n",
    "\n",
    "while True:\n",
    "    print(state)\n",
    "    scores = game.scores(state)\n",
    "    print(f\"scores: {scores[1]}, {scores[-1]}\")\n",
    "    if player ==1:\n",
    "        neutral_state = game.change_perspective(state, player)\n",
    "        mtcs_probs = mcts2.search(neutral_state)\n",
    "        action = np.argmax(mtcs_probs)\n",
    "        if action == game.action_size -1:\n",
    "            pass_count +=1\n",
    "        else:\n",
    "            pass_count = 0\n",
    "        print(\"bot 1- \", str(action))\n",
    "    \n",
    "    else:\n",
    "        neutral_state = game.change_perspective(state, player)\n",
    "        mtcs_probs = mcts.search(neutral_state)\n",
    "        action = np.argmax(mtcs_probs)\n",
    "        if action == game.action_size -1:\n",
    "            pass_count +=1\n",
    "        else:\n",
    "            pass_count = 0\n",
    "        print(\"bot 2- \", str(action))\n",
    "        \n",
    "    state = game.get_next_state(state, action, player)\n",
    "\n",
    "\n",
    "    previous_state = state\n",
    "    \n",
    "    if pass_count == 2:\n",
    "        print(state)\n",
    "        print(\"Game over\")\n",
    "        value = game.winner(state)\n",
    "        print(f\"scores: {scores[1]}, {scores[-1]}\")\n",
    "        if value != 0:\n",
    "            print(f\"{value} wins\")\n",
    "        else:\n",
    "            print(\"Draw\")\n",
    "        break\n",
    "    \n",
    "    player = game.get_opponent(player)\n",
    "                            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teste 3: Modelo treinado vs Humano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]]\n",
      "scores: 0, 5.5\n",
      "valid moves [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49]\n",
      "minha 49\n",
      "pass count 1\n",
      "[[0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]]\n",
      "scores: 0, 5.5\n",
      "valid moves [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49]\n",
      "minha 49\n",
      "pass count 2\n",
      "[[0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]]\n",
      "Game over\n",
      "scores: 0, 5.5\n",
      "-1 wins\n"
     ]
    }
   ],
   "source": [
    "state = game.get_initial_state()\n",
    "previous_state = game.get_initial_state()\n",
    "\n",
    "player = 1\n",
    "\n",
    "pass_count = 0\n",
    "\n",
    "def value_scores(scores):\n",
    "    if scores[1] > scores[-1]:\n",
    "        return 1\n",
    "    elif scores[1] < scores[-1]:\n",
    "        return -1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "while True:\n",
    "    print(state)\n",
    "    scores = game.scores(state)\n",
    "    print(f\"scores: {scores[1]}, {scores[-1]}\")\n",
    "\n",
    "    if player != 1:\n",
    "        valid_moves = game.get_valid_moves(state, previous_state)\n",
    "        print(\"valid moves\", [i for i in range(game.action_size) if valid_moves[i] == 1])\n",
    "        action = int(input(\"Enter action: \"))\n",
    "        print(\"minha \" +str(action))\n",
    "        if action == game.action_size -1:\n",
    "            pass_count +=1\n",
    "        else:\n",
    "            pass_count = 0\n",
    "        print(\"pass count\", str( pass_count))\n",
    "        if valid_moves[action] == 0:\n",
    "            print(\"Invalid move\")\n",
    "            continue\n",
    "\n",
    "    \n",
    "    else:\n",
    "        valid_moves = game.get_valid_moves(state, previous_state)\n",
    "        print(\"valid moves\", [i for i in range(game.action_size) if valid_moves[i] == 1])\n",
    "        action = int(input(\"Enter action: \"))\n",
    "        print(\"minha \" +str(action))\n",
    "        if action == game.action_size -1:\n",
    "            pass_count +=1\n",
    "        else:\n",
    "            pass_count = 0\n",
    "        print(\"pass count\", str( pass_count))\n",
    "        if valid_moves[action] == 0:\n",
    "            print(\"Invalid move\")\n",
    "            continue\n",
    "        \n",
    "    state = game.get_next_state(state, action, player)\n",
    "    \n",
    "    previous_state = state\n",
    "    \n",
    "    if pass_count == 2:\n",
    "        print(state)\n",
    "        print(\"Game over\")\n",
    "        value = game.winner(state)\n",
    "        print(f\"scores: {scores[1]}, {scores[-1]}\")\n",
    "        if value != 0:\n",
    "            print(f\"{value} wins\")\n",
    "        else:\n",
    "            print(\"Draw\")\n",
    "        break\n",
    "\n",
    "    \n",
    "    player = game.get_opponent(player)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
